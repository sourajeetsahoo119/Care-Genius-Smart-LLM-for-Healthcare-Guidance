{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Text Input"
      ],
      "metadata": {
        "id": "fXPaPPGgEjhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def get_medical_response_from_text(text_input):\n",
        "    \"\"\"\n",
        "    Generate a personalized medical analysis based on user-provided text.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Patient query: \" + text_input + \"\\n\"\n",
        "        \"Provide a detailed, personalized medical analysis and advice based on the \"\n",
        "        \"information provided.\"\n",
        "    )\n",
        "    try:\n",
        "       # Map model choices to verified model identifiers\n",
        "        model_mapping = {\n",
        "            \"small\": \"medalpaca/medalpaca-7b\",  # Smaller, easier to run\n",
        "            \"medium\": \"StanfordAIMI/Med-Flamingo-7B\",  # Good balance\n",
        "            \"large\": \"meditron-7b\"  # This might need specific access\n",
        "        }\n",
        "\n",
        "        model_name = model_mapping.get(model_choice, \"medalpaca/medalpaca-7b\")\n",
        "        print(f\"Loading {model_name}...\")\n",
        "\n",
        "        # Check device\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Load tokenizer and model with error handling\n",
        "\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "                device_map=\"auto\" if device == \"cuda\" else None,\n",
        "                low_cpu_mem_usage=True,\n",
        "                load_in_8bit=True if device == \"cuda\" else False\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {model_name}: {e}\")\n",
        "            print(\"Falling back to smaller model...\")\n",
        "            return run_medical_model(user_input, \"small\")\n",
        "\n",
        "        # Create pipeline\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=0 if device == \"cuda\" else -1,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        "        )\n",
        "        # Generate response\n",
        "        print(\"Medical Recommendation :\")\n",
        "        outputs = pipe(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Extract response\n",
        "        full_response = outputs[0]['generated_text']\n",
        "        response = full_response.split(\"### Assistant Response:\")[-1].strip()\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def main():\n",
        "    print(\"Enter your medical query (text input):\")\n",
        "    user_input = input(\">> \")\n",
        "    result = get_medical_response_from_text(user_input)\n",
        "    print(\"\\nMedical Recommendation:\")\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHmaDGa8Bq_s",
        "outputId": "ceef8635-d90a-4a73-bbc9-3735da2e21a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your medical query (text input):\n",
            ">> As I have asthma, what are the food products that l need to avoid in order to keep my balance keep it short.\n",
            "\n",
            "Medical Recommendation:\n",
            "As an asthma patient, it's important to be mindful of certain foods that may trigger or worsen your symptoms. Some common food triggers for asthma include:\n",
            "\n",
            "1. Dairy products: Some people with asthma may be sensitive to dairy products, which can cause inflammation and increase mucus production in the airways.\n",
            "\n",
            "2. Processed and fried foods: These foods are often high in trans fats and other unhealthy ingredients that can promote inflammation in the body and worsen asthma symptoms.\n",
            "\n",
            "3. Excessive salt: High sodium intake can lead to fluid retention and may worsen breathing difficulties in asthma patients.\n",
            "\n",
            "4. Sulfites and preservatives: Foods and drinks that contain sulfites and other preservatives can trigger asthma symptoms in some individuals.\n",
            "\n",
            "5. Foods high in histamines: Histamine-rich foods like fermented dairy products, cured meats, and aged cheeses may trigger asthma symptoms in certain individuals.\n",
            "\n",
            "It's important to pay attention to your individual triggers and work with your healthcare provider to identify and avoid foods that may worsen your asthma symptoms. Keeping a food diary can help you track your intake and identify potential triggers. Additionally, maintaining a healthy, balanced diet rich in fruits, vegetables, whole grains, and lean proteins can help support your overall respiratory health. Always consult with your healthcare provider for personalized advice and recommendations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PDF/Document\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3USIp4KEpjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvSoEHeFFcbQ",
        "outputId": "0cc392ef-3f1c-46fd-f571-1443424a751c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF document using PyPDF2.\n",
        "    \"\"\"\n",
        "    text_content = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page in pdf_reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text_content += page_text + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF file: {str(e)}\")\n",
        "    return text_content\n",
        "\n",
        "def get_medical_response_from_pdf(text_input):\n",
        "    \"\"\"\n",
        "    Uses the Meditron 7B to generate a medical analysis based on extracted PDF text.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Extracted medical document content: \" + text_input + \"\\n\"\n",
        "        \"Provide a detailed analysis and personalized recommendations based on the content.\"\n",
        "    )\n",
        "    try:\n",
        "       # Map model choices to verified model identifiers\n",
        "        model_mapping = {\n",
        "            \"small\": \"medalpaca/medalpaca-7b\",  # Smaller, easier to run\n",
        "            \"medium\": \"StanfordAIMI/Med-Flamingo-7B\",  # Good balance\n",
        "            \"large\": \"meditron-7b\"  # This might need specific access\n",
        "        }\n",
        "\n",
        "        model_name = model_mapping.get(model_choice, \"medalpaca/medalpaca-7b\")\n",
        "        print(f\"Loading {model_name}...\")\n",
        "\n",
        "        # Check device\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Load tokenizer and model with error handling\n",
        "\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "                device_map=\"auto\" if device == \"cuda\" else None,\n",
        "                low_cpu_mem_usage=True,\n",
        "                load_in_8bit=True if device == \"cuda\" else False\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {model_name}: {e}\")\n",
        "            print(\"Falling back to smaller model...\")\n",
        "            return run_medical_model(user_input, \"small\")\n",
        "\n",
        "        # Generate response\n",
        "        print(\"Medical Recommendation :\")\n",
        "        outputs = pipe(\n",
        "            prompt,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Extract response\n",
        "        full_response = outputs[0]['generated_text']\n",
        "        response = full_response.split(\"### Assistant Response:\")[-1].strip()\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "def main():\n",
        "    pdf_file_path = input(\"Enter the path to your medical PDF document: \")\n",
        "    extracted_text = extract_text_from_pdf(pdf_file_path)\n",
        "    if extracted_text:\n",
        "        print(\"\\nExtracted Text from PDF (first 500 characters):\")\n",
        "        print(extracted_text[:500] + \"...\\n\")\n",
        "        result = get_medical_response_from_pdf(extracted_text)\n",
        "        print(\"Medical Analysis from PDF:\")\n",
        "        print(result)\n",
        "    else:\n",
        "        print(\"No text extracted from the PDF.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2elQYypEIPK",
        "outputId": "f94fe4b4-e22d-4405-a4f4-4028b03ea860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to your medical PDF document: /content/genome-report-example.pdf\n",
            "\n",
            "Extracted Text from PDF (first 500 characters):\n",
            "_______________________________ _____________________________________________________________________________________________________________________________ __________________________  \n",
            "Laboratory for Molecular Medicine                                                                                                                                                                 Matthew S. Lebo, Ph.D., FACMG, Director  \n",
            "https://www.massgeneralbrigham.org/en/research -and-innovation/centers -and-p...\n",
            "\n",
            "Medical Analysis from PDF:\n",
            "The extracted medical document provides detailed genetic information regarding monogenic disease findings, risk alleles, carrier status variants, and pharmacogenomic associations for an individual. \n",
            "\n",
            "1. Monogenic Disease Findings: The test did not identify any variants with the potential to cause monogenic disease in the individual.\n",
            "\n",
            "2. Risk Alleles: No reportable risk alleles were identified in the genetic analysis.\n",
            "\n",
            "3. Carrier Status Variants: The individual was found to be a carrier for three autosomal recessive conditions:\n",
            "   - Primary ciliary dyskinesia, due to variants in CCDC40 and DNAH11\n",
            "   - Short/branched chain acyl-CoA dehydrogenase deficiency, due to a variant in ACADSB\n",
            "\n",
            "4. Pharmacogenomic Associations: The report includes detailed information on genetic variants that may impact drug responses.\n",
            "\n",
            "5. Variant Interpretation: Each genetic variant is analyzed for its pathogenicity and potential impact on health. Detailed explanations are provided for each variant identified.\n",
            "\n",
            "6. Disease Information: Information is given about the associated genetic conditions and the potential impact on health for each identified variant.\n",
            "\n",
            "7. Familial and Reproductive Risk: The report also provides information on the risk to the individual's children or future offspring based on their carrier status for specific genetic variants.\n",
            "\n",
            "8. Recommendations: Genetic counseling is recommended for the individual and their relatives based on the identified carrier status variants.\n",
            "\n",
            "9. Coverage Summary: Details about the coverage of sequencing performed and the limitations of the test are included in the report.\n",
            "\n",
            "10. References: The report includes references to scientific studies supporting the findings and interpretations provided.\n",
            "\n",
            "Overall, the report provides comprehensive genetic information that can be used to guide personalized healthcare decisions and potentially improve health outcomes for the individual and their family. It is recommended to discuss the results with a healthcare provider for further guidance and interpretation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vision"
      ],
      "metadata": {
        "id": "7MEBWbgyGfeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai pymupdf opencv-python pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh4gddJKHbD_",
        "outputId": "2f9bbd11-d022-4ec9-9f25-a21028654d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai) (3.11.12)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai) (1.18.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF for PDF processing\n",
        "import base64  # For encoding images\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "def setup_meditron_model():\n",
        "    try:\n",
        "        model_name = \"medalpaca/medalpaca-7b\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False)\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error loading Meditron model: {str(e)}\")\n",
        "\n",
        "def extract_mri_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        if not os.path.exists(pdf_path):\n",
        "            return \"Error: File not found.\"\n",
        "\n",
        "        text = \"\"\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page in doc:\n",
        "                page_text = page.get_text(\"text\")\n",
        "                if \"MRI\" in page_text or \"magnetic resonance imaging\" in page_text.lower():\n",
        "                    text += page_text + \"\\n\"\n",
        "\n",
        "        return text.strip() if text.strip() else \"Error: No MRI-related text detected in the PDF.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting text from PDF: {str(e)}\"\n",
        "\n",
        "def encode_image(image_path):\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
        "    except Exception as e:\n",
        "        return f\"Error encoding image: {str(e)}\"\n",
        "\n",
        "def analyze_with_meditron(pipe, prompt, max_tokens=500):\n",
        "    try:\n",
        "        formatted_prompt = f\"\"\"### Medical Analysis Task:\n",
        "You are a medical expert analyzing MRI reports and scans. Provide detailed, professional medical insights.\n",
        "\n",
        "### Input:\n",
        "{prompt}\n",
        "\n",
        "### Medical Analysis:\n",
        "\"\"\"\n",
        "        response = pipe(\n",
        "            formatted_prompt,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=pipe.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        full_text = response[0]['generated_text']\n",
        "        analysis = full_text.split(\"### Medical Analysis:\")[-1].strip()\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating analysis: {str(e)}\"\n",
        "\n",
        "def analyze_medical_document(file_path):\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            return \"Error: The file does not exist. Check the path.\"\n",
        "\n",
        "        pipe = setup_meditron_model()\n",
        "\n",
        "        if file_path.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "            image_data = encode_image(file_path)\n",
        "            if \"Error\" in image_data:\n",
        "                return image_data\n",
        "\n",
        "            prompt = f\"\"\"MRI Image Analysis Request:\n",
        "\n",
        "                I have an MRI scan image (base64 encoded: {image_data[:100]}...). Please provide detailed analysis including:\n",
        "                1. General interpretation guidelines for this type of scan\n",
        "                2. Common findings in similar MRI scans\n",
        "                3. Potential clinical significance\n",
        "                4. Recommended next steps\n",
        "                \"\"\"\n",
        "            return analyze_with_meditron(pipe, prompt, max_tokens=600)\n",
        "\n",
        "        elif file_path.lower().endswith(\".pdf\"):\n",
        "            extracted_text = extract_mri_text_from_pdf(file_path)\n",
        "            if \"Error\" in extracted_text:\n",
        "                return extracted_text\n",
        "\n",
        "            prompt = f\"\"\"Extracted MRI text from document:\n",
        "{extracted_text}\n",
        "\n",
        "Analyze this MRI report and provide a detailed medical recommendation, including any potential conditions, observations, and next steps.\n",
        "\"\"\"\n",
        "            return analyze_with_meditron(pipe, prompt, max_tokens=700)\n",
        "\n",
        "        else:\n",
        "            return \"Unsupported file type. Please provide a JPG, PNG, or PDF.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing the document: {str(e)}\"\n",
        "\n",
        "def main():\n",
        "    file_path = input(\"Enter the path to your medical file (image or PDF): \").strip()\n",
        "    result = analyze_medical_document(file_path)\n",
        "    print(\"\\nðŸ“„ Medical Analysis Result:\")\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "izpgW_zVWWjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0944fb-5a14-4b3b-867d-9bab3e1aff40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to your medical file (image or PDF): /content/mrinew.jpeg\n",
            "\n",
            "ðŸ“„ **Medical Analysis Result:**\n",
            "The provided MRI scan images include sections of both the knee and the lower extremity (possibly the ankle), presented in various planesâ€”primarily the sagittal and coronal planes, which are common orientations used for detailed evaluation of joint structures.\n",
            "\n",
            "### Analysis:\n",
            "\n",
            "1. **Knee Analysis (Sagittal and Coronal Views)**\n",
            "   - **Structures Visible**: Femur, tibia, fibula, knee joint space, menisci, and surrounding soft tissues.\n",
            "   - **Menisci**: The menisci appear to be intact without evident degeneration or tearing. Both medial and lateral menisci display normal shape and signal intensity.\n",
            "   - **Ligaments**: Cruciate ligaments (ACL and PCL) and collateral ligaments (MCL and LCL) are not clearly visible in every image but where visible, they do not show signs of rupture or significant alteration.\n",
            "   - **Cartilage**: The articular cartilage covering the ends of the femur and tibia shows consistent thickness and signal without signs of significant wear or osteoarthritis.\n",
            "   - **Bone**: No obvious signs of bone lesions, fractures, or significant degenerative changes.\n",
            "   - **Joint Space**: Appears maintained, which is indicative of absence of advanced arthritis.\n",
            "\n",
            "2. **Lower Extremity Analysis (possibly ankle but not clearly identifiable from current views)**\n",
            "   - The images are less clear regarding which specific joint or part of the lower extremity is displayed. Assuming it to be the ankle:\n",
            "   - **Bones**: The tibia, fibula, and talus should be visible in typical ankle MRIs.\n",
            "   - **Ligaments and Tendons**: Look for integrity and any signs of tears or abnormal signal which could suggest sprains or tendinopathy.\n",
            "   - **Cartilage and Joint Space**: Evaluation of cartilage health and joint spacing is crucial for diagnosing arthritis or other degenerative changes.\n",
            "\n",
            "### Recommendations:\n",
            "- **Clinical Correlation**: It's essential to correlate these findings with clinical symptoms and history for a comprehensive evaluation.\n",
            "- **Further Imaging**: If any symptoms of instability, pain, or restricted movement are reported, additional imaging (like MRI sequences focusing on specific ligaments or cartilage) or diagnostic modalities might be required.\n",
            "- **Follow-Up**: Routine monitoring if there's a history of joint-related issues, or immediate follow-up if new symptoms arise.\n",
            "\n",
            "### Conclusion:\n",
            "The MRI scans do not reveal any immediate abnormalities in the knee or lower extremity sections presented. However, clinical correlation is crucial, and further focused imaging may be required based on the patient's symptoms or medical history.\n"
          ]
        }
      ]
    }
  ]
}